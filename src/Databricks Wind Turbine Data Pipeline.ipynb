{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239cd129-d641-43c5-b86d-a777886ec875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "116e9133-7259-40c9-af1a-b6fabd805085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define ADLS Gen2 Storage Variables\n",
    "STORAGE_ACCOUNT_NAME = \"your_adls_account\"\n",
    "CONTAINER_NAME = \"your_container\"\n",
    "MOUNT_NAME = \"your_mount_name\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8ff928e-e5a7-4be6-9d4c-61940ebcf15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define ADLS Paths\n",
    "BRONZE_PATH = f\"dbfs:/mnt/{MOUNT_NAME}/bronze/wind_turbine_data\"\n",
    "SILVER_PATH = f\"dbfs:/mnt/{MOUNT_NAME}/silver/wind_turbine_data\"\n",
    "GOLD_PATH = f\"dbfs:/mnt/{MOUNT_NAME}/gold/wind_turbine_summary\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52cfb862-0b2d-4e2e-a798-92f64a33fb83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount ADLS Gen2 Storage\n",
    "def mount_adls():\n",
    "    \"\"\"Mounts Azure Data Lake Storage (ADLS Gen2) to Databricks.\"\"\"\n",
    "    configs = {\n",
    "        \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "        \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "        \"fs.azure.account.oauth2.client.id\": \"your_client_id\",\n",
    "        \"fs.azure.account.oauth2.client.secret\": \"your_client_secret\",\n",
    "        \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/your_tenant_id/oauth2/token\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        dbutils.fs.mount(\n",
    "            source=f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\",\n",
    "            mount_point=f\"/mnt/{MOUNT_NAME}\",\n",
    "            extra_configs=configs\n",
    "        )\n",
    "        print(f\"ADLS mounted successfully at `/mnt/{MOUNT_NAME}`\")\n",
    "    except Exception as e:\n",
    "        print(f\"ADLS Mount Failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41682244-6886-4129-af15-568731877a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark Session (Databricks auto-initializes Spark)\n",
    "spark = SparkSession.builder.appName(\"WindTurbineProcessing\").config(\"spark.sql.extensions\",\n",
    "                                                                     \"io.delta.sql.DeltaSparkSessionExtension\").config(\n",
    "    \"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7316e8-a9c4-48a6-b2cb-3ec3e2abdf2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_bronze_layer() -> DataFrame:\n",
    "    \"\"\"Reads raw CSV data from the Bronze Layer in DBFS.\"\"\"\n",
    "    try:\n",
    "        return spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(f\"{BRONZE_PATH}/*.csv\")\n",
    "    except AnalysisException as e:\n",
    "        print(f\"Error reading data from Bronze Layer: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eb22b35-b0fd-4f30-ba85-402acfaff310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def impute_missing_values(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Handles missing values by filling numeric columns with median and categorical with 'Unknown'.\"\"\"\n",
    "    numeric_cols = [col for col, dtype in df.dtypes if dtype in ['int', 'double']]\n",
    "\n",
    "    # Compute median values for all numeric columns in one pass\n",
    "    median_values = {col: df.approxQuantile(col, [0.5], 0.05)[0] for col in numeric_cols}\n",
    "\n",
    "    # Apply median imputation\n",
    "    df = df.fillna(median_values)\n",
    "    df = df.fillna({'turbine_id': 'Unknown'})  # Handle missing categorical values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26936e9a-33a6-4e6d-a4ec-e843b05c43a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def detect_outliers(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Detects outliers using the Interquartile Range (IQR) method.\"\"\"\n",
    "    cols_to_check = ['wind_speed', 'power_output', 'wind_direction']\n",
    "\n",
    "    # Compute Q1, Q3, and IQR in a single pass\n",
    "    quantiles = df.select(\n",
    "        *[F.percentile_approx(c, [0.25, 0.75], 10000).alias(f\"{c}_quantiles\") for c in cols_to_check]).first()\n",
    "\n",
    "    bounds = {col: {'lower': quantiles[f\"{col}_quantiles\"][0] - 1.5 * (\n",
    "                quantiles[f\"{col}_quantiles\"][1] - quantiles[f\"{col}_quantiles\"][0]),\n",
    "                    'upper': quantiles[f\"{col}_quantiles\"][1] + 1.5 * (\n",
    "                                quantiles[f\"{col}_quantiles\"][1] - quantiles[f\"{col}_quantiles\"][0])}\n",
    "              for col in cols_to_check}\n",
    "\n",
    "    # Apply outlier detection in a single batch operation\n",
    "    for col in cols_to_check:\n",
    "        df = df.withColumn(f\"{col}_is_outlier\",\n",
    "                           (F.col(col) < bounds[col]['lower']) | (F.col(col) > bounds[col]['upper']))\n",
    "\n",
    "    df = df.withColumn(\"is_any_outlier\",\n",
    "                       F.array_contains(F.array(*[F.col(f\"{col}_is_outlier\") for col in cols_to_check]), True))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "580106c9-bf68-4ddd-8c5b-98848f0a88b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def detect_anomalies(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Detects anomalies in power output using standard deviation thresholding.\"\"\"\n",
    "    stats_df = df.groupBy(\"turbine_id\").agg(\n",
    "        F.mean(\"power_output\").alias(\"mean_output\"),\n",
    "        F.stddev(\"power_output\").alias(\"std_dev\")\n",
    "    )\n",
    "\n",
    "    # Join statistics with the main DataFrame\n",
    "    df = df.join(stats_df, on=\"turbine_id\", how=\"left\")\n",
    "\n",
    "    # Detect anomalies in a batch operation\n",
    "    df = df.withColumn(\"is_anomaly\",\n",
    "                       (F.col(\"power_output\") < (F.col(\"mean_output\") - 2 * F.col(\"std_dev\"))) |\n",
    "                       (F.col(\"power_output\") > (F.col(\"mean_output\") + 2 * F.col(\"std_dev\"))))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9f5fe68-7f68-4c41-bfed-e57980882548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_silver_layer(df: DataFrame):\n",
    "    \"\"\"Writes the cleaned DataFrame to the Silver Layer in Delta format.\"\"\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH)\n",
    "    print(f\"Cleaned data successfully written to Silver Layer (Delta): {SILVER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b8e0d9a-d549-4fa7-9544-89eb94bbbc82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def summarize_statistics(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Generates summary statistics and writes them to the Gold Layer in Delta format.\"\"\"\n",
    "    df_with_date = df.withColumn(\"date\", F.to_date(\"timestamp\"))\n",
    "\n",
    "    summary_stats = df_with_date.groupBy(\"date\", \"turbine_id\").agg(\n",
    "        F.min(\"power_output\").alias(\"min_power_output\"),\n",
    "        F.max(\"power_output\").alias(\"max_power_output\"),\n",
    "        F.avg(\"power_output\").alias(\"avg_power_output\")\n",
    "    ).orderBy(\"date\")\n",
    "\n",
    "    summary_stats.write.format(\"delta\").mode(\"overwrite\").save(GOLD_PATH)\n",
    "    print(f\"Summary statistics successfully written to Gold Layer (Delta): {GOLD_PATH}\")\n",
    "\n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc9d4e0d-4cae-44ac-8093-eca9470dbdfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# **RUN THE PIPELINE**\n",
    "try:\n",
    "    print(\"Starting Wind Turbine Data Pipeline...\\n\")\n",
    "\n",
    "    # Step 1: Read Data\n",
    "    df = read_bronze_layer()\n",
    "    if df is None:\n",
    "        raise Exception(\"Data read failed!\")\n",
    "\n",
    "    # Step 2: Handle Missing Values\n",
    "    df = impute_missing_values(df)\n",
    "\n",
    "    # Step 3: Detect Outliers\n",
    "    df = detect_outliers(df)\n",
    "\n",
    "    # Step 4: Detect Anomalies\n",
    "    df = detect_anomalies(df)\n",
    "\n",
    "    # Step 5: Write to Silver Layer (Delta Format)\n",
    "    write_to_silver_layer(df)\n",
    "\n",
    "    # Step 6: Compute & Write Summary Statistics to Gold Layer (Delta Format)\n",
    "    summary_stats = summarize_statistics(df)\n",
    "\n",
    "    print(\"Pipeline execution completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline execution failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Wind Turbine Data Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}